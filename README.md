<img width="530" alt="Screenshot 2025-03-18 at 4 09 58â€¯PM" src="https://github.com/user-attachments/assets/06a8f1a3-4b2e-4b17-999c-6fb67616456f" />

## The pipeline is built to:

1. **Ingest Data**  
   Fetch data from the GitHub API using **Apache Airflow** and stream it using **Apache Kafka**.

2. **Process Data**  
   Perform real-time processing and analysis of the data using **Apache Spark** for streaming.
   Perform batch analysis of the data using **Apache Spark SQl** .

4. **Store Data**  
   Store the processed data in **Apache Cassandra** for efficient querying and retrieval.

5. **Containerize the Pipeline**  
   Use **Docker** to containerize the entire pipeline.
   

